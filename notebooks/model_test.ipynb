{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import List, Tuple, Union\n",
    "import numpy as np\n",
    "from itertools import zip_longest\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import logging\n",
    "from rdkit import Chem\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/chenlidong/polyAttn/models\")\n",
    "sys.path.append(\"/home/chenlidong/polyAttn/utils\")\n",
    "# import polygnn\n",
    "import chem_utils\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# polygnn测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps ={\n",
    "    \"capacity\" : 2,\n",
    "    \"activation\": nn.functional.leaky_relu,\n",
    "    \"readout_dim\": 128\n",
    "}\n",
    "model = polygnn.polygnn(32,7,True,hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features, edge_features, edge_indices = chem_utils.get_feature(\"[*]Oc1ccc(-c2ccc(C([*])=O)cc2)cc1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(\n",
    "    x = torch.tensor(node_features, dtype=torch.float),\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).T,\n",
    "    edge_weight = torch.tensor(edge_features, dtype=torch.float),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "            dataset=[data],\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4337, 0.3888, 0.3650, 0.3436, 0.3988, 0.3499, 0.3956, 0.4133, 0.3567,\n",
      "         0.4352, 0.3738, 0.3076, 0.3832, 0.3693, 0.4158, 0.4409, 0.4127, 0.4084,\n",
      "         0.4287, 0.4251, 0.3337, 0.4168, 0.4429, 0.3431, 0.4172, 0.3630, 0.4217,\n",
      "         0.4064, 0.3803, 0.3957, 0.4111, 0.3503, 0.4047, 0.4475, 0.3495, 0.3449,\n",
      "         0.3784, 0.3999, 0.3801, 0.3518, 0.4274, 0.3092, 0.3732, 0.3583, 0.3823,\n",
      "         0.3584, 0.4099, 0.4279, 0.3385, 0.3373, 0.4120, 0.3693, 0.3263, 0.4335,\n",
      "         0.3913, 0.4095, 0.3489, 0.3001, 0.4066, 0.3830, 0.3599, 0.3715, 0.4065,\n",
      "         0.4032, 0.3953, 0.4147, 0.3725, 0.4038, 0.3840, 0.4478, 0.3282, 0.3981,\n",
      "         0.3390, 0.3768, 0.4021, 0.4305, 0.4000, 0.3825, 0.3874, 0.4016, 0.3292,\n",
      "         0.3710, 0.3279, 0.4633, 0.3882, 0.3455, 0.4211, 0.4184, 0.3750, 0.3709,\n",
      "         0.3644, 0.3426, 0.4080, 0.4180, 0.3483, 0.3365, 0.3140, 0.2962, 0.3640,\n",
      "         0.4033, 0.3791, 0.4103, 0.4069, 0.4254, 0.3285, 0.3350, 0.2872, 0.3729,\n",
      "         0.3825, 0.3926, 0.3977, 0.3607, 0.4197, 0.4316, 0.4576, 0.3801, 0.4005,\n",
      "         0.4207, 0.3973, 0.3989, 0.3322, 0.4043, 0.3445, 0.3671, 0.3954, 0.4078,\n",
      "         0.4106, 0.3942]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(model(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# polymer-chemprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/chenlidong/polymer-chemprop-master/chemprop\")\n",
    "from features import mol2graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = \"[*:1]c1ccc([*:2])c2nsnc12.[*:3]c1cc([*:4])nc(OC)c1N|0.25|0.75|<1-3:0.25:0.25<1-4:0.25:0.25<2-3:0.25:0.25<2-4:0.25:0.25<1-2:0.25:0.25<3-4:0.25:0.25<1-1:0.25:0.25<2-2:0.25:0.25<3-3:0.25:0.25<4-4:0.25:0.25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mol2graph = mol2graph([smiles])\n",
    "f_atoms, f_bonds, w_atoms, w_bonds, a2b, b2a, b2revb, a_scope, b_scope, _  = batch_mol2graph.get_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2a = batch_mol2graph.get_a2a()\n",
    "b2b = batch_mol2graph.get_b2b()\n",
    "# ea = torch.tensor(valid_df.Ea.to_list(),dtype=torch.float32)\n",
    "# ip = torch.tensor(valid_df.IP.to_list(),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for idx,(a_scope_i,b_scope_i) in enumerate(zip(a_scope,b_scope)):\n",
    "    edge_index = [[],[]]\n",
    "    b2a_i = b2a[b_scope_i[0]:b_scope_i[0]+b_scope_i[1]]\n",
    "    b2revb_i = b2revb[b_scope_i[0]:b_scope_i[0]+b_scope_i[1]]\n",
    "\n",
    "    for i in range(b2a_i.size(0)):\n",
    "        edge_index[0].append(b2a_i[i].item()-a_scope_i[0])\n",
    "        edge_index[1].append(b2a_i[b2revb_i[i]-b_scope_i[0]]-a_scope_i[0])\n",
    "\n",
    "    data = Data(\n",
    "        x = f_atoms[a_scope_i[0]:a_scope_i[0]+a_scope_i[1]],\n",
    "        edge_index=torch.tensor(edge_index,dtype=torch.long),\n",
    "        edge_attr = f_bonds[b_scope_i[0]:b_scope_i[0]+b_scope_i[1]],\n",
    "        w_atoms = w_atoms[a_scope_i[0]:a_scope_i[0]+a_scope_i[1]],\n",
    "        w_bonds = w_bonds[b_scope_i[0]:b_scope_i[0]+b_scope_i[1]],\n",
    "        # a2b = a2b[a_scope_i[0]:a_scope_i[0]+a_scope_i[1]],\n",
    "        # b2a = b2a[b_scope_i[0]:b_scope_i[0]+b_scope_i[1]],\n",
    "        b2revb = b2revb[b_scope_i[0]:b_scope_i[0]+b_scope_i[1]]-1,\n",
    "        # a2a = a2a[a_scope_i[0]:a_scope_i[0]+a_scope_i[1]],\n",
    "        # b2b = b2b[b_scope_i[0]:b_scope_i[0]+b_scope_i[1]],\n",
    "        # ea = ea[idx],\n",
    "        # ip = ip[idx]\n",
    "    )\n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Tuple\n",
    "from functools import reduce\n",
    "from rdkit import Chem\n",
    "from args import TrainArgs\n",
    "from features import BatchMolGraph, get_atom_fdim, get_bond_fdim\n",
    "from nn_utils import get_activation_function, index_select_ND\n",
    "import torch.nn.init as init\n",
    "\n",
    "class MPNEncoder(nn.Module):\n",
    "    \"\"\"An :class:`MPNEncoder` is a message passing neural network for encoding a molecule.\"\"\"\n",
    "\n",
    "    def __init__(self, args: TrainArgs, atom_fdim: int, bond_fdim: int):\n",
    "        \"\"\"\n",
    "        :param args: A :class:`~chemprop.args.TrainArgs` object containing model arguments.\n",
    "        :param atom_fdim: Atom feature vector dimension.\n",
    "        :param bond_fdim: Bond feature vector dimension.\n",
    "        \"\"\"\n",
    "        super(MPNEncoder, self).__init__()\n",
    "        self.atom_fdim = atom_fdim                  # atom feature len\n",
    "        self.bond_fdim = bond_fdim                  # bond feature len\n",
    "        self.atom_messages = args.atom_messages     # 是否以原子为中心传递\n",
    "        self.hidden_size = args.hidden_size         # 300\n",
    "        self.depth = args.depth                     # 消息传递的步数\n",
    "        self.dropout = args.dropout\n",
    "        self.undirected = args.undirected           # False\n",
    "        self.device = torch.device('cpu')\n",
    "        self.aggregation = args.aggregation         # mean\n",
    "        self.aggregation_norm = args.aggregation_norm   # 100\n",
    "        self.atom_messages = True\n",
    "        self.directed = not self.atom_messages\n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Activation\n",
    "        self.act_func = get_activation_function(args.activation)    # Relu\n",
    "\n",
    "        # Cached zeros\n",
    "        self.cached_zero_vector = nn.Parameter(torch.zeros(self.hidden_size), requires_grad=False)\n",
    "\n",
    "        # Input\n",
    "        input_dim = self.atom_fdim if self.atom_messages else self.bond_fdim\n",
    "        self.W_i = nn.Linear(input_dim, self.hidden_size)\n",
    "\n",
    "        if self.atom_messages:\n",
    "            w_h_input_size = self.hidden_size + self.bond_fdim\n",
    "        else:\n",
    "            w_h_input_size = self.hidden_size\n",
    "\n",
    "        # Shared weight matrix across depths (default)\n",
    "        self.W_h = nn.Linear(w_h_input_size, self.hidden_size)\n",
    "\n",
    "        self.W_o = nn.Linear(self.atom_fdim + self.hidden_size, self.hidden_size)\n",
    "\n",
    "        init.constant_(self.W_i.weight, 0.01)\n",
    "        init.constant_(self.W_i.bias, 1)\n",
    "        init.constant_(self.W_h.weight, 0.01)\n",
    "        init.constant_(self.W_h.bias, 1)\n",
    "        init.constant_(self.W_o.weight, 0.01)\n",
    "        init.constant_(self.W_o.bias, 1)\n",
    "\n",
    "    def forward(self,\n",
    "                mol_graph: BatchMolGraph,\n",
    "                atom_descriptors_batch: List[np.ndarray] = None) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Encodes a batch of molecular graphs.\n",
    "\n",
    "        :param mol_graph: A :class:`~chemprop.features.featurization.BatchMolGraph` representing\n",
    "                          a batch of molecular graphs.\n",
    "        :param atom_descriptors_batch: A list of numpy arrays containing additional atomic descriptors\n",
    "        :return: A PyTorch tensor of shape :code:`(num_molecules, hidden_size)` containing the encoding of each molecule.\n",
    "        \"\"\"\n",
    "        f_atoms, f_bonds, w_atoms, w_bonds, a2b, b2a, b2revb, \\\n",
    "        a_scope, b_scope, degree_of_polym = mol_graph.get_components(atom_messages=False)\n",
    "\n",
    "        f_atoms, f_bonds, w_atoms, w_bonds, a2b, b2a, b2revb = f_atoms.to(self.device), f_bonds.to(self.device), \\\n",
    "                                                               w_atoms.to(self.device), w_bonds.to(self.device), \\\n",
    "                                                               a2b.to(self.device), b2a.to(self.device), \\\n",
    "                                                               b2revb.to(self.device)\n",
    "\n",
    "        if self.atom_messages:\n",
    "            a2a = mol_graph.get_a2a().to(self.device)\n",
    "\n",
    "\n",
    "        # Input\n",
    "        if self.atom_messages:\n",
    "            input = self.W_i(f_atoms)                                   # [num_atoms , hidden_size]\n",
    "        else:\n",
    "            input = self.W_i(f_bonds)                                   # [num_bonds , hidden_size]\n",
    "        message = self.act_func(input)                                  # [num_bonds / num_atoms , hidden_size]\n",
    "        \n",
    "        \n",
    "        # Message passing                                               \n",
    "        for depth in range(self.depth - 1):\n",
    "            if self.undirected:\n",
    "                message = (message + message[b2revb]) / 2\n",
    "\n",
    "            if self.atom_messages:\n",
    "                nei_a_message = index_select_ND(message, a2a)           # [num_atoms , max_num_bonds , hidden_size] 某一原子，其周围所有原子的特征\n",
    "                nei_f_bonds = index_select_ND(f_bonds, a2b)             # [num_atoms , max_num_bonds , bond_fdim]\n",
    "                nei_message = torch.cat((nei_a_message, nei_f_bonds), dim=2)  # [num_atoms , max_num_bonds , hidden + bond_fdim]\n",
    "                message = nei_message.sum(dim=1)                        # [num_atoms , hidden + bond_fdim]\n",
    "            else:\n",
    "                # m(a1 -> a2) = [sum_{a0 \\in nei(a1)} m(a0 -> a1)] - m(a2 -> a1)\n",
    "                # message      a_message = sum(nei_a_message)      rev_message\n",
    "                nei_a_message = index_select_ND(message, a2b)           # [num_atoms , max_num_bonds , hidden]\n",
    "                nei_a_weight = index_select_ND(w_bonds, a2b)            # [num_atoms , max_num_bonds]\n",
    "                # weight nei_a_message based on edge weights\n",
    "                # m(a1 -> a2) = [sum_{a0 \\in nei(a1)} m(a0 -> a1) * weight(a0 -> a1)] - m(a2 -> a1)\n",
    "                # message      a_message = dot(nei_a_message,nei_a_weight)      rev_message\n",
    "                nei_a_message = nei_a_message * nei_a_weight[..., None]  # [num_atoms , max_num_bonds , hidden]\n",
    "                a_message = nei_a_message.sum(dim=1)  # num_atoms x hidden\n",
    "                rev_message = message[b2revb]  # num_bonds x hidden]\n",
    "                # print(rev_message.shape)\n",
    "                # print(nei_a_weight[..., None].shape)\n",
    "                message = a_message[b2a] - rev_message * w_bonds[..., None]       # [num_bonds , hidden]\n",
    "            print(message)\n",
    "            message = self.W_h(message)\n",
    "            message = self.act_func(input + message)                    # [num_bonds/num_atoms , hidden]  skip connection\n",
    "            # message = self.dropout_layer(message)                       # [num_bonds/num_atoms , hidden]\n",
    "            # print(message)\n",
    "            \n",
    "            break\n",
    "        a2x = a2a if self.atom_messages else a2b\n",
    "        nei_a_message = index_select_ND(message, a2x)                   # [num_bonds/num_atoms , max_num_bonds , hidden]\n",
    "        nei_a_weight = index_select_ND(w_bonds, a2x)                    # [num_bonds/num_atoms , max_num_bonds]\n",
    "        # # weight messages\n",
    "        nei_a_message = nei_a_message * nei_a_weight[..., None]         # [num_bonds/num_atoms , max_num_bonds , hidden]\n",
    "        a_message = nei_a_message.sum(dim=1)                            # [num_bonds/num_atoms , hidden]\n",
    "        \n",
    "        a_input = torch.cat([f_atoms, a_message], dim=1)                # [num_bonds/num_atoms , hidden + f_atom]\n",
    "        # print(a_input)\n",
    "        # atom_hiddens = self.act_func(self.W_o(a_input))                 # [num_bonds/num_atoms , hidden]\n",
    "        # atom_hiddens = self.dropout_layer(atom_hiddens)                 # [num_bonds/num_atoms , hidden]\n",
    "        # print(a_input)\n",
    "\n",
    "        # # Readout\n",
    "        # mol_vecs = []\n",
    "        # for i, (a_start, a_size) in enumerate(a_scope):\n",
    "        #     if a_size == 0:\n",
    "        #         mol_vecs.append(self.cached_zero_vector)\n",
    "        #     else:\n",
    "        #         cur_hiddens = atom_hiddens.narrow(0, a_start, a_size)\n",
    "        #         mol_vec = cur_hiddens  # (num_atoms, hidden_size)\n",
    "        #         w_atom_vec = w_atoms.narrow(0, a_start, a_size)\n",
    "        #         # if input are polymers, weight atoms from each repeating unit according to specified monomer fractions\n",
    "        #         # weight h by atom weights (weights are all 1 for non-polymer input)\n",
    "        #         mol_vec = w_atom_vec[..., None] * mol_vec\n",
    "        #         # weight each atoms at readout\n",
    "        #         if self.aggregation == 'mean':\n",
    "        #             mol_vec = mol_vec.sum(dim=0) / w_atom_vec.sum(dim=0)  # if not --polymer, w_atom_vec.sum == a_size\n",
    "        #         elif self.aggregation == 'sum':\n",
    "        #             mol_vec = mol_vec.sum(dim=0)\n",
    "        #         elif self.aggregation == 'norm':\n",
    "        #             mol_vec = mol_vec.sum(dim=0) / self.aggregation_norm\n",
    "\n",
    "        #         # if input are polymers, multiply mol vectors by degree of polymerization\n",
    "        #         # if not --polymer, Xn is 1\n",
    "        #         mol_vec = degree_of_polym[i] * mol_vec\n",
    "\n",
    "        #         mol_vecs.append(mol_vec)\n",
    "\n",
    "        # mol_vecs = torch.stack(mol_vecs, dim=0)  # (num_molecules, hidden_size)\n",
    "\n",
    "        # return mol_vecs  # num_molecules x hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric import nn as pnn\n",
    "from typing import List, Union, Tuple\n",
    "from functools import reduce\n",
    "from rdkit import Chem\n",
    "from args import TrainArgs\n",
    "from features import BatchMolGraph, get_atom_fdim, get_bond_fdim\n",
    "from nn_utils import get_activation_function, index_select_ND\n",
    "class MPNEncoder_PyG(pnn.MessagePassing):\n",
    "    \"\"\"An :class:`MPNEncoder` is a message passing neural network for encoding a molecule.\"\"\"\n",
    "\n",
    "    def __init__(self, args: TrainArgs, atom_fdim: int, bond_fdim: int):\n",
    "        \"\"\"\n",
    "        :param args: A :class:`~chemprop.args.TrainArgs` object containing model arguments.\n",
    "        :param atom_fdim: Atom feature vector dimension.\n",
    "        :param bond_fdim: Bond feature vector dimension.\n",
    "        \"\"\"\n",
    "        super(MPNEncoder_PyG, self).__init__(aggr=\"add\")\n",
    "        self.atom_fdim = atom_fdim                  # atom feature len\n",
    "        self.bond_fdim = bond_fdim                  # bond feature len\n",
    "        self.atom_messages = args.atom_messages     # 是否以原子为中心传递\n",
    "        self.hidden_size = args.hidden_size         # 300\n",
    "        self.depth = args.depth                     # 消息传递的步数\n",
    "        self.dropout = args.dropout\n",
    "        self.device = torch.device('cpu')\n",
    "        self.aggregation = args.aggregation         # mean\n",
    "        self.aggregation_norm = args.aggregation_norm   # 100\n",
    "        self.atom_messages = True\n",
    "        self.directed = not self.atom_messages\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # Activation\n",
    "        self.act_func = get_activation_function(args.activation)    # Relu\n",
    "\n",
    "        # Cached zeros\n",
    "        self.cached_zero_vector = nn.Parameter(torch.zeros(self.hidden_size), requires_grad=False)\n",
    "\n",
    "        # Input\n",
    "        input_dim = self.atom_fdim if self.atom_messages else self.bond_fdim\n",
    "        self.W_i = nn.Linear(input_dim, self.hidden_size)\n",
    "\n",
    "        if self.atom_messages:\n",
    "            w_h_input_size = self.hidden_size + self.bond_fdim\n",
    "        else:\n",
    "            w_h_input_size = self.hidden_size\n",
    "\n",
    "        # Shared weight matrix across depths (default)\n",
    "        self.W_h = nn.Linear(w_h_input_size, self.hidden_size)\n",
    "        self.W_o = nn.Linear(self.atom_fdim + self.hidden_size, self.hidden_size)\n",
    "\n",
    "\n",
    "        init.constant_(self.W_i.weight, 0.01)\n",
    "        init.constant_(self.W_i.bias, 1)\n",
    "        init.constant_(self.W_h.weight, 0.01)\n",
    "        init.constant_(self.W_h.bias, 1)\n",
    "        init.constant_(self.W_o.weight, 0.01)\n",
    "        init.constant_(self.W_o.bias, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr,b2revb,w_atoms,w_bonds, batch):\n",
    "        a_message = x\n",
    "        b_message = edge_attr\n",
    "        if not self.directed:\n",
    "            input = self.W_i(a_message)\n",
    "            a_message = self.act_func(input)              # [num_atoms , hidden_size]\n",
    "        else:\n",
    "            input = self.W_i(b_message)\n",
    "            b_message = self.act_func(input)              # [num_bonds , hidden_size]\n",
    "\n",
    "        for depth in range(self.depth - 1):\n",
    "            message = self.propagate(\n",
    "                edge_index      =   edge_index,\\\n",
    "                x               =   a_message,\\\n",
    "                edge_attr       =   b_message,\\\n",
    "                b2revb          =   b2revb,\\\n",
    "                w_atoms         =   w_atoms,\\\n",
    "                w_bonds         =   w_bonds,\\\n",
    "                skip_connection =   False,\\\n",
    "                first           =   x)\n",
    "            print(message)\n",
    "            if self.directed:\n",
    "                b_message = self.act_func(self.W_h(message)+input)\n",
    "            else:\n",
    "                a_message = self.act_func(self.W_h(message)+input)\n",
    "                # print(a_message)\n",
    "            \n",
    "            break\n",
    "\n",
    "        return self.propagate(\n",
    "                edge_index      =   edge_index,\\\n",
    "                x               =   a_message,\\\n",
    "                edge_attr       =   b_message,\\\n",
    "                b2revb          =   b2revb,\\\n",
    "                w_atoms         =   w_atoms,\\\n",
    "                w_bonds         =   w_bonds,\\\n",
    "                skip_connection =   True,\\\n",
    "                first           =   x)\n",
    "\n",
    "    def update(self, aggr_out,edge_index,edge_attr,b2revb,w_bonds,skip_connection,first):\n",
    "        if not skip_connection:\n",
    "            if self.directed:\n",
    "                message = aggr_out[edge_index[0]] - edge_attr[b2revb] * w_bonds[...,None]\n",
    "            else:\n",
    "                message = aggr_out\n",
    "        else:\n",
    "            message = torch.cat([first, aggr_out], dim=1)\n",
    "        return message\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index,edge_attr,b2revb,w_bonds,skip_connection,first):\n",
    "        # x_i x_j   从i节点到j节点的边是edge_index\n",
    "        if not skip_connection:\n",
    "            if self.directed:\n",
    "                nei_a_message = edge_attr * w_bonds[..., None]\n",
    "            else:\n",
    "                nei_a_message = torch.cat((x_j,edge_attr),dim=1)\n",
    "        else:\n",
    "            if self.directed:\n",
    "                nei_a_message = edge_attr * w_bonds[..., None]\n",
    "            else:\n",
    "                nei_a_message = x_j * w_bonds[edge_index[0]][..., None]\n",
    "        return nei_a_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pyg = MPNEncoder_PyG(args=TrainArgs, atom_fdim=get_atom_fdim(),bond_fdim=get_bond_fdim())\n",
    "model = MPNEncoder(args=TrainArgs, atom_fdim=get_atom_fdim(),bond_fdim=get_bond_fdim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.4272, 6.4272, 6.4272,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [2.1424, 2.1424, 2.1424,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [2.1424, 2.1424, 2.1424,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [1.0616, 1.0616, 1.0616,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [3.2038, 3.2038, 3.2038,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0712, 1.0712, 1.0712,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ScatterAddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    # print(batch.x)\n",
    "    model(batch_mol2graph)\n",
    "    model_pyg(batch.x,batch.edge_index,batch.edge_attr,batch.b2revb,batch.w_atoms,batch.w_bonds,batch.batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    x  torch.Size([18, 133])\n",
      "    edge_index  torch.Size([2, 54])\n",
      "    edge_attr  torch.Size([54, 147])\n",
      "    w_atoms  torch.Size([18])\n",
      "    w_bonds  torch.Size([54])\n",
      "    batch  torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(f\"\\\n",
    "    x  {batch.x.shape}\\n\\\n",
    "    edge_index  {batch.edge_index.shape}\\n\\\n",
    "    edge_attr  {batch.edge_attr.shape}\\n\\\n",
    "    w_atoms  {batch.w_atoms.shape}\\n\\\n",
    "    w_bonds  {batch.w_bonds.shape}\\n\\\n",
    "    batch  {batch.batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  0,  6,  1,  2,  2,  3,  2,  4,  4,  5,  5,  6,  6,  7,  8,  9,\n",
       "          8, 13,  9, 10, 10, 11, 11, 12, 12, 13,  0,  8,  0, 12,  4,  8,  4, 12,\n",
       "          0,  4,  8, 12,  0,  4,  8, 12],\n",
       "        [ 1,  0,  6,  0,  2,  1,  3,  2,  4,  2,  5,  4,  6,  5,  7,  6,  9,  8,\n",
       "         13,  8, 10,  9, 11, 10, 12, 11, 13, 12,  8,  0, 12,  0,  8,  4, 12,  4,\n",
       "          4,  0, 12,  8,  0,  4,  8, 12]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([\n",
    "        [1.0000e+00, 1.9700e+02, 9.0800e+02],\n",
    "        [2.0000e+00, 4.4000e+01, 2.0000e+02],\n",
    "        [3.0000e+00, 3.9000e+01, 2.1300e+02],\n",
    "        [4.0000e+00, 2.0400e+02, 9.1400e+02],\n",
    "        [5.0000e+00, 6.1000e+01, 2.6800e+02],\n",
    "        [6.0000e+00, 3.3000e+01, 6.8000e+01],\n",
    "        [7.0000e+00, 2.8000e+01, 6.4000e+01],\n",
    "        [8.0000e+00, 2.8000e+01, 6.8000e+01],\n",
    "        [9.0000e+00, 7.3000e+01, 2.5900e+02],\n",
    "        [1.0000e+01, 2.4300e+02, 1.0070e+03],\n",
    "        [1.1000e+01, 1.0600e+02, 4.7300e+02],\n",
    "        [1.2000e+01, 2.2700e+02, 9.7000e+02],\n",
    "        [1.3000e+01, 9.6000e+01, 3.3700e+02],\n",
    "        [1.4000e+01, 9.8000e+01, 2.1800e+02],\n",
    "        [1.5000e+01, 6.0000e+01, 1.3000e+02],\n",
    "        [1.6000e+01, 3.0000e+01, 6.7000e+01],\n",
    "        [1.7000e+01, 1.1700e+02, 3.6300e+02],\n",
    "        [1.8000e+01, 4.2000e+01, 9.4000e+01]])\n",
    "b = torch.tensor([\n",
    "    [1.0000e+00, 1.9700e+02, 9.0800e+02],\n",
    "        [2.0000e+00, 4.4000e+01, 2.0000e+02],\n",
    "        [3.0000e+00, 3.9000e+01, 2.1300e+02],\n",
    "        [4.0000e+00, 2.0400e+02, 9.1400e+02],\n",
    "        [5.0000e+00, 6.1000e+01, 2.6800e+02],\n",
    "        [6.0000e+00, 3.3000e+01, 6.8000e+01],\n",
    "        [7.0000e+00, 2.8000e+01, 6.4000e+01],\n",
    "        [8.0000e+00, 2.8000e+01, 6.8000e+01],\n",
    "        [9.0000e+00, 7.3000e+01, 2.5900e+02],\n",
    "        [1.0000e+01, 2.4300e+02, 1.0070e+03],\n",
    "        [1.1000e+01, 1.0600e+02, 4.7300e+02],\n",
    "        [1.2000e+01, 2.2700e+02, 9.7000e+02],\n",
    "        [1.3000e+01, 9.6000e+01, 3.3700e+02],\n",
    "        [1.4000e+01, 9.8000e+01, 2.1800e+02],\n",
    "        [1.5000e+01, 6.0000e+01, 1.3000e+02],\n",
    "        [1.6000e+01, 3.0000e+01, 6.7000e+01],\n",
    "        [1.7000e+01, 1.1700e+02, 3.6300e+02],\n",
    "        [1.8000e+01, 4.2000e+01, 9.4000e+01]\n",
    "])\n",
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyG",
   "language": "python",
   "name": "py_38_torch_113_pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a330dc2ace33e4380118d1f67cb64e7e74e50a0f1d111ab0831bb66ce0df072"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
